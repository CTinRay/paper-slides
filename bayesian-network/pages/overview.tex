\begin{frame}
  \frametitle{Overview}
  \begin{itemize}
  \item RNN learnt by learning distribution of weights.
  \item Better performance on language modeling and image caption generation than original LSTM.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Motivation}
  \begin{itemize}
  \item Regularization by posteriori
  \item Expression of uncertainty
  \item (An ensemble of many networks)
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Variational Approximation}
  \begin{itemize}
  \item Assume that network weights $\theta$ follows a prior distribution $P(\theta)$.
  \item From data $D$, we want to learn 
    \begin{align*}
      P(\theta | D) = \frac{P(D | \theta) P(\theta)}{\int_\theta P(D | \theta) P(\theta) \mathrm{d}\theta }
    \end{align*}
  \item But $\int_\theta P(D | \theta) P(\theta) \mathrm{d}\theta$ is intractable.
  \item Assume that $\theta \sim \mathcal{N}(\mu, \sigma^2)$, what about learn $\mu, \sigma$ to estimate $P(\theta | D)$ with $q(\theta | \mu, \sigma^2)$ (Abbreviated to $q(\theta)$.
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Variational Free Energy}
  \begin{itemize}
  \item To approximate with $q(w | \theta)$, we want to minimize
    \begin{align*}
       & \mathrm{KL}[q(\theta) || P(\theta | D)] \\
      =& \int_\theta q(\theta) \log \frac{q(\theta)}{P(\theta | D)} \mathrm{d} \theta \\
      =& \int_\theta q(\theta) \log \frac{q(\theta) P(D)}{P(D, \theta)} \mathrm{d} \theta \\
      =& \int_\theta q(\theta) \log \frac{q(\theta)}{P(D, \theta)} \mathrm{d} \theta +
      \int_w q(\theta) \log P(D) \mathrm{d} \theta\\
      =& \int_\theta q(\theta) \log \frac{q(\theta)}{P(\theta) P(D | \theta)} \mathrm{d} w + \log P(D) \\
      =& \int_\theta q(\theta) \log \frac{q(\theta)}{P(\theta)} \mathrm{d} \theta
         - \int_\theta q(\theta) \log P(D | \theta) \mathrm{d} \theta + \log P(D) \\
      =& \underbrace{
             \overbrace{\mathrm{KL}[q(\theta) || P(\theta)]}^{\text{complexity cost}}
             \overbrace{-\mathop{\mathbb{E}_{q(\theta)}}[\log P(D | \theta)]}^{\text{likelihood cost}}
         }_{\text{Variational Free Energy, to minimize}}
         + \underbrace{\log P(D)}_{\text{constant}}
    \end{align*}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Gradient of Variational Free Energy - $\theta$}
  % Assume that $\theta = (\mu, \sigma)$, $q(w | \theta) = \phi(w | \mu, \sigma^2)$, $q(\epsilon) = \phi(\epsilon)$, $w = \mu + \epsilon \circ \sigma$
  \begin{align*}
     & \mathrm{KL}[q(\theta) || P(\theta)] - \mathop{\mathbb{E}_{q(\theta)}}[\log P(D | \theta)] \\
    =& \int_w q(\theta) [\log q(\theta) - \log P(\theta) - \log P(D | \theta)] \mathrm{d} \theta \\
    =& \mathop{\mathbb{E}_\theta }[\log q(\theta) - \log P(\theta) - \log P(D | \theta)]
  \end{align*}
  That is,
  \begin{align*}
     & \frac{\partial}{\partial \theta} \mathrm{KL}[q(\theta) || P(\theta)] - \mathop{\mathbb{E}_{q(\theta)}}[\log P(D | \theta)] \\
    =& \mathop{\mathbb{E}_\epsilon }
       [\underbrace{\frac{\partial}{\partial \theta} \log q(\theta)
       - \frac{\partial}{\partial \theta} \log P(\theta)}_{g^{KL}_\theta}
       \underbrace{- \frac{\partial}{\partial \theta} \log P(D | \theta)}_{\text{original gradient $g$}}]
  \end{align*}
  
\end{frame}