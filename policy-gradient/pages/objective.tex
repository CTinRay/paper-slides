\begin{frame}
  \frametitle{Objective Function 1}
  \begin{itemize}
  \item Two ways to formulate how good a agent (policy $\pi$) is.
  \item Long-term expected reward per step:
    \begin{align*}
         \rho(\pi)
      &= \lim_{n \to \infty} \frac{1}{n} E\{r_1 + r_2 + r_3 + \cdots + r_n | \pi\} \\
      &= \sum_s d^{\pi} (s) \sum_a \pi(s, a) \mathcal{R}_s^a
    \end{align*}
    where $d^\pi(s) = \lim_{t \to \infty} Pr\{s_t = s | s_0 = s, a_0 = a, \pi\}$
    \footnote{In other words, let $M_{ij} = Pr\{s_{t+1} = s^j | s_t = s^i, \pi\}$, it can be prooved that $\lim_{t \to \infty} M^t x$ converges to $y$. And $y_i$ here is exactly $d^{\pi}(s^i)$.}
  \item Value of state-action pair for $\pi$:
    \begin{equation*}
      Q^\pi(s, a) = \sum_{t=1}^{\infty} E\{r_t - \rho(\pi) | s_0 = s, a_0 = a, \pi \}
    \end{equation*}
  \end{itemize}
\end{frame}


\begin{frame}
  \frametitle{Objective Function 2}
  \begin{itemize}
  \item Two ways to formulate how good a agent (policy $\pi$) is. Here is the second way.
  \item Long-term reward obtained from start state $s_0$:
    \begin{equation*}
      \rho(\pi) = E \left\{\sum_{t=1}^{\infty} \gamma^{t-1}r_t | s_0, \pi \right\}
    \end{equation*}
  \item Value of state-action pair for $\pi$:
    \begin{equation*}
      Q^\pi(s, a) = E \left\{\sum_{k=1}^\infty \gamma^{k-1} r_{t+k} | s_t = s, a_t = a, \pi \right\}
    \end{equation*}
  \end{itemize}
\end{frame}