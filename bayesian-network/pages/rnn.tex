\section{Backprop Through Time}

\begin{frame}
  \frametitle{Loss for RNN}
  Rewrite
  \begin{equation*}
    \mathcal{L}(\theta) = 
             \mathrm{KL}[q(\theta) || P(\theta)]
             -\mathop{\mathbb{E}_{q(\theta)}}[\log P(D | \theta)]
  \end{equation*}
  as
  \begin{align}
    \mathcal{L}(\theta) &=
                          - \mathbb{E}_{q(\theta)}\left[\log \prod_{b=1}^B \prod_{c=1}^{C} p(y^{(b,c)}|\theta, x^{(b,c)}) \right]
                          \nonumber \\
                        &
                          + KL[q(\theta) || p(\theta)]
  \end{align}

\end{frame}


\begin{frame}
  \frametitle{Bayes by Backprop for RNNs}
  \begin{algorithm}[H]
    \caption{Bayes by Backprop for RNNs}
        \label{alg:rnnbbb}
    \begin{algorithmic}
        \STATE{Sample $\epsilon \sim \mathcal{N}(0,I)$, $\epsilon \in \mathbb{R}^d$.}
        \STATE{Set network parameters to $\theta = \mu + \sigma\epsilon$.}
        \STATE{Sample a minibatch of truncated sequences $(x,y)$.}
        \STATE{Do forward propagation and backpropagation as normal on minibatch.}
        \STATE{Let $g$ be the gradient with respect to $\theta$ from backpropagation.}
        \STATE{Let $g^{KL}_\theta, g^{KL}_\mu, g^{KL}_\sigma$ be the gradients of $\log \mathcal{N}(\theta|\mu, \sigma) - \log p(\theta)$ w.r.t. $\theta$, $\mu$ and $\sigma$ respectively.}
        \STATE{Update $\mu$ according to the gradient $\frac{g + \frac{1}{C}g^{KL}_\theta}{B} + \frac{g^{KL}_\mu}{B C}$.}
        \STATE{Update $\sigma$ according to the gradient $\left(\frac{g + \frac{1}{C} g^{KL}_\theta}{B}\right) \epsilon + \frac{g^{KL}_\sigma}{B C}$.}
    \end{algorithmic}
\end{algorithm}

\end{frame}